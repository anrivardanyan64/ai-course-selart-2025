{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5c7544",
   "metadata": {},
   "source": [
    "Module 5,Session 2  \n",
    "Activation Functions and Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b265f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155d23b",
   "metadata": {},
   "source": [
    "Exercise 1- Activation Functions\n",
    "\n",
    "We are given the following inputs:\n",
    "z = [-5, -1, 0, 1, 5]\n",
    "\n",
    "Below are the outputs for different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29a1ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00669285, 0.26894142, 0.5       , 0.73105858, 0.99330715]),\n",
       " array([-0.9999092 , -0.76159416,  0.        ,  0.76159416,  0.9999092 ]),\n",
       " array([0, 0, 0, 1, 5]),\n",
       " array([-0.5, -0.1,  0. ,  1. ,  5. ]),\n",
       " array([-0.03346425, -0.26894142,  0.        ,  0.73105858,  4.96653575]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([-5, -1, 0, 1, 5])\n",
    "\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "tanh = np.tanh(z)\n",
    "relu = np.maximum(0, z)\n",
    "leaky_relu = np.where(z > 0, z, 0.1 * z)\n",
    "swish = sigmoid * z\n",
    "\n",
    "sigmoid, tanh, relu, leaky_relu, swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667843ab",
   "metadata": {},
   "source": [
    "Sigmoid\n",
    "Outputs values between 0 and 1\n",
    "Not zero-centered\n",
    "\n",
    "Tanh\n",
    "Outputs values between -1 and 1\n",
    "Zero-centered\n",
    "\n",
    "ReLU\n",
    "Outputs 0 for negative values\n",
    "Very computationally simple\n",
    "\n",
    "Leaky ReLU\n",
    "Like ReLU but allows small negative values\n",
    "\n",
    "Swish\n",
    "Smooth, modern activation\n",
    "Used in advanced neural networks\n",
    "\n",
    "Answers\n",
    "Zero-centered output - Тanh\n",
    "Computationally simplest - ReLU\n",
    "Preferred for deep neural networks - ReLU / Leaky ReLU / Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6461b",
   "metadata": {},
   "source": [
    "Exercise 2 - Forward Pass\n",
    "\n",
    "Input X = [0.5, 0.3]\n",
    "\n",
    "Weights W1 =[[0.2, 0.4],[0.7, 0.9]]\n",
    "Bias B1 = [0.1, 0.2]\n",
    "\n",
    "Weights W2 =[[0.6],[0.8]]\n",
    "Bias B2 = [0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b9c32",
   "metadata": {},
   "source": [
    "Step 1 - Hidden Layer Weighted Sum (Z1)\n",
    "\n",
    "Z1 = X * W1 + B1\n",
    "\n",
    "Z1₁ = (0.5 * 0.2) + (0.3 * 0.7) + 0.1  \n",
    "Z1₁ = 0.1 + 0.21 + 0.1 = 0.41\n",
    "\n",
    "Z1₂ = (0.5 * 0.4) + (0.3 * 0.9) + 0.2  \n",
    "Z1₂ = 0.2 + 0.27 + 0.2 = 0.67\n",
    "\n",
    "Z1 = [0.41, 0.67]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be0a29",
   "metadata": {},
   "source": [
    "Step 2 - Apply ReLU\n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "A1 = [0.41, 0.67]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c63ba",
   "metadata": {},
   "source": [
    "Step 3 - Output Layer Weighted Sum (Z2)\n",
    "\n",
    "Z2 = A1 * W2 + B2\n",
    "\n",
    "Z2 = (0.41 * 0.6) + (0.67 * 0.8) + 0.3  \n",
    "Z2 = 0.246 + 0.536 + 0.3 = 1.082"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed5c3a",
   "metadata": {},
   "source": [
    "Step 4 -  Sigmoid Activation\n",
    "ŷ ≈ 0.747\n",
    "\n",
    "Final Output - The network predicts approximately = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d13b2c",
   "metadata": {},
   "source": [
    "Exercise 3 - Architecture Design\n",
    "\n",
    "1.Input Layer neurons  \n",
    "32 × 32 = 1024 neurons  \n",
    "Because each pixel is one input feature.\n",
    "\n",
    "2.Hidden Layer activation\n",
    "ReLU\n",
    "It is computationally efficient and helps avoid vanishing gradients.\n",
    "\n",
    "3.Output Layer neurons \n",
    "3 neurons\n",
    "One for each class: Cat, Dog, Bird.\n",
    "\n",
    "4.Output Layer activation\n",
    "Softmax\n",
    "Because this is a multi-class classification problem and Softmax outputs probabilities\n",
    "that sum to 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
